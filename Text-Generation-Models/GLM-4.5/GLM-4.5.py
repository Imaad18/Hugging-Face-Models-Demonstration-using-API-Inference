# -*- coding: utf-8 -*-
"""Rough Work

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o_C3wYoy2rQmIxB-FYEELYIacqPc0mGq
"""

# ============================================
# ğŸ¤– AI Task Assistant using GLM-4.5 on Hugging Face
# Compatible with Google Colab
# ============================================

# ğŸ› ï¸ Install required packages
!pip install -q huggingface_hub

# ğŸ“¦ Import libraries
import os
from huggingface_hub import InferenceClient
from google.colab import userdata

# ============================================
# ğŸ” Securely fetch Hugging Face Token from Colab Secrets
# ============================================
HF_TOKEN = userdata.get('HF_TOKEN')
if not HF_TOKEN:
    raise ValueError("âŒ Error: HF_TOKEN not found in Colab secrets.")

# Initialize Hugging Face inference client
client = InferenceClient(
    provider="novita",
    api_key=HF_TOKEN
)

# ============================================
# ğŸš€ Demo Function: Showcase GLM-4.5 Abilities
# ============================================
def demo_glm_model():
    """Demonstrate GLM-4.5 capabilities with various prompts"""
    demo_prompts = [
        {"task": "Basic Q&A", "prompt": "What is the capital of France?"},
        {"task": "Creative Writing", "prompt": "Write a short haiku about artificial intelligence."},
        {"task": "Problem Solving", "prompt": "Solve this step by step: If a train travels 120 km in 2 hours, what is its average speed?"},
        {"task": "Code Generation", "prompt": "Write a Python function to find the factorial of a number using recursion."},
        {"task": "Text Analysis", "prompt": "Analyze the sentiment of this text: 'I'm absolutely thrilled about the new project launch!'"}
    ]

    print("\n\u2728 GLM-4.5 Model Capabilities Demo\n")

    for demo in demo_prompts:
        print(f"ğŸ” Task: {demo['task']}")
        print(f"ğŸ“ Prompt: {demo['prompt']}")
        print("-" * 50)

        try:
            completion = client.chat.completions.create(
                model="zai-org/GLM-4.5",
                messages=[{"role": "user", "content": demo['prompt']}],
                max_tokens=500,
                temperature=0.7
            )

            response = completion.choices[0].message.content
            print(f"ğŸ¤– Response: {response}\n")

        except Exception as e:
            print(f"âŒ Error: {str(e)}\n")

        print("="*60 + "\n")

# ============================================
# ğŸ’¬ Interactive Chat Function
# ============================================
def interactive_chat():
    """Chat with GLM-4.5 interactively in Colab"""
    print("ğŸš€ Interactive Chat with GLM-4.5 (type 'quit' to exit)\n")

    history = []

    while True:
        user_input = input("You: ").strip()

        if user_input.lower() in ['quit', 'exit', 'bye']:
            print("ğŸ‘‹ Goodbye!")
            break

        if not user_input:
            continue

        history.append({"role": "user", "content": user_input})

        try:
            completion = client.chat.completions.create(
                model="zai-org/GLM-4.5",
                messages=history,
                max_tokens=500,
                temperature=0.7
            )

            response = completion.choices[0].message.content
            print(f"GLM-4.5: {response}\n")

            history.append({"role": "assistant", "content": response})

        except Exception as e:
            print(f"âŒ Error: {str(e)}\n")

# ============================================
# ğŸ§ª Run Demo and Optional Chat
# ============================================
if __name__ == "__main__":
    demo_glm_model()

    try:
        run_chat = input("Would you like to try interactive chat? (y/n): ").strip().lower()
        if run_chat.startswith('y'):
            interactive_chat()
    except EOFError:
        print("âš ï¸ Input not available in this environment.")