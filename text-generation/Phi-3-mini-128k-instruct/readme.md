![phi](https://github.com/user-attachments/assets/eac35168-5259-417f-8722-01c01066f6ad)


# Phi-3 Mini (128k) Instruct - Chat-ready Model

[![Hugging Face](https://img.shields.io/badge/HuggingFace-Model-yellow?logo=huggingface)](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct)  
**Model:** `microsoft/Phi-3-mini-128k-instruct`  
**Type:** Instruction-tuned, causal language model  
**Context Length:** 128,000 tokens  
**Architecture:** Transformer decoder-only (GPT-like)  
**License:** MIT  

---

## ğŸ“Œ Overview

`Phi-3-mini-128k-instruct` is a 3.8 billion parameter **instruction-tuned** model developed by Microsoft, part of the Phi-3 family. It is trained to follow natural language instructions and optimized for conversational tasks, math reasoning, code generation, and long-context understanding (up to **128k tokens**).

This model uses the **ChatML** format (`<|system|>`, `<|user|>`, `<|assistant|>`) for prompt construction, enabling clear role-based multi-turn conversations similar to ChatGPT.

---

## ğŸš€ Key Features

- ğŸ” **128k context length** â€“ handle long documents and multi-turn conversations
- ğŸ’¬ **Instruction-tuned** â€“ suitable for chat, reasoning, question-answering
- ğŸ§  **Reasoning & math** â€“ good performance on equations, logic, and structured answers
- ğŸ§¾ **ChatML prompt formatting** â€“ clearly distinguish system, user, and assistant roles
- ğŸ§ª Lightweight â€“ 3.8B parameters, suitable for consumer GPUs (12â€“16GB VRAM)

---

## ğŸ› ï¸ Installation & Setup

```bash
pip install transformers accelerate
```
